{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zygf9ds-Mx4q"
      },
      "outputs": [],
      "source": [
        "!pip install rasterio\n",
        "!apt install imagemagick"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJEaBePGZUfh"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import requests\n",
        "import os\n",
        "import cv2\n",
        "import urllib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from subprocess import call\n",
        "import rasterio\n",
        "from osgeo import gdal\n",
        "from os import path\n",
        "from rasterio.mask import mask\n",
        "from collections import Counter\n",
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8G9kYBk7ZQ4M"
      },
      "outputs": [],
      "source": [
        "# API Key used to access the API\n",
        "os.environ['PL_API_KEY']='5affbba496b5487b8ca1ba4e89a2cd8d'\n",
        "PLANET_API_KEY = os.getenv('PL_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EI4gcF6aYGv5",
        "outputId": "b875734c-e884-467f-e8f1-8f2a50286d00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KV-OXmeEZUmW"
      },
      "outputs": [],
      "source": [
        "# Setup Planet API\n",
        "URL = \"https://api.planet.com/data/v1\"\n",
        "session = requests.Session()\n",
        "session.auth = (PLANET_API_KEY, \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyYcR1v9ZVQJ",
        "outputId": "b15eae23-0ac7-4734-b26c-40faef0a87d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://api.planet.com/data/v1/stats\n"
          ]
        }
      ],
      "source": [
        "stats_url = \"{}/stats\".format(URL)\n",
        "print(stats_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPIiR9cAl4uK"
      },
      "outputs": [],
      "source": [
        "# Iterate through images to find the highest and lowest latidude and longitude coordinates\n",
        "def get_corners(features): \n",
        "  longs = []\n",
        "  lats = []\n",
        "  for f in features:\n",
        "    coords = f[\"geometry\"][\"coordinates\"][0]\n",
        "    longs.extend([coords[0][0], coords[1][0], coords[2][0], coords[3][0]])\n",
        "    lats.extend([coords[0][1], coords[1][1], coords[2][1], coords[3][1]])\n",
        "\n",
        "  return max(lats), min(lats), max(longs), min(longs)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kE7Z5rdfRm-b"
      },
      "outputs": [],
      "source": [
        "# Save search results returned by Planet\n",
        "def save_pngs(features, path, search_id, res):\n",
        "  png_imgs = []\n",
        "  for feature in features:\n",
        "    id = feature[\"id\"]\n",
        "    thumb_url = feature[\"_links\"][\"thumbnail\"]\n",
        "    x = feature[\"properties\"][\"origin_x\"]\n",
        "    y = feature[\"properties\"][\"origin_y\"]\n",
        "    resolution = feature[\"properties\"][\"pixel_resolution\"]\n",
        "    columns = feature[\"properties\"][\"columns\"]\n",
        "\n",
        "    img_path = path + id + \".png\"\n",
        "    # Get the thumbnail from the url by including the API key and the width\n",
        "    img_url = thumb_url + \"?api_key=5affbba496b5487b8ca1ba4e89a2cd8d&width=\" + str(res)\n",
        "    resp = urllib.request.urlopen(img_url)\n",
        "    img = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
        "    img = cv2.imdecode(img, cv2.IMREAD_COLOR)\n",
        "    os.chdir(path)\n",
        "    sys.path.append(path + search_id + '/')\n",
        "    cv2.imwrite(id+\".png\", img)\n",
        "    png_imgs.append([img, img_path, id, x, y, resolution, columns])  \n",
        "  \n",
        "  return png_imgs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ju7blSwurrip"
      },
      "outputs": [],
      "source": [
        "# Make imgs transparent tifs\n",
        "def save_transparent_tifs(input_path):\n",
        "  cmds = []\n",
        "  tif_imgs = []\n",
        "\n",
        "  for img in os.listdir(input_path):\n",
        "    if img[-3:] == \"png\":\n",
        "      planet_id = img[:-4]\n",
        "      planet_tif = \"{}_transparent.tif\".format(planet_id)\n",
        "      cmds.append(\"convert -trim {}.png {}\".format(planet_id, planet_tif))\n",
        "      tif_imgs.append(planet_tif)\n",
        "\n",
        "  for cmd in cmds:\n",
        "      call(cmd.split(\" \"))\n",
        "\n",
        "  return tif_imgs\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNMDBofqFwAr"
      },
      "outputs": [],
      "source": [
        "# Crop images to show only the current search area\n",
        "def crop_img(geom, merged_img, img_path):\n",
        "  geom_crop = [{'type': 'Polygon', 'coordinates': [[(geom['coordinates'][0][0][0], geom['coordinates'][0][0][1]),\n",
        "                                                    (geom['coordinates'][0][1][0], geom['coordinates'][0][1][1]),\n",
        "                                                    (geom['coordinates'][0][2][0], geom['coordinates'][0][2][1]),\n",
        "                                                    (geom['coordinates'][0][3][0], geom['coordinates'][0][3][1]),\n",
        "                                                    (geom['coordinates'][0][4][0], geom['coordinates'][0][4][1]),\n",
        "    ]]}]\n",
        "\n",
        "  if os.path.exists(merged_img):\n",
        "    try:\n",
        "        with rasterio.open(merged_img) as src:\n",
        "          out_image, out_transform = mask(src, geom_crop, crop=True)\n",
        "        out_meta = src.meta.copy()\n",
        "        # save the resulting raster  \n",
        "        out_meta.update({\"driver\": \"GTiff\",\n",
        "            \"height\": out_image.shape[1],\n",
        "            \"width\": out_image.shape[2],\n",
        "        \"transform\": out_transform})\n",
        "\n",
        "        # save the final image to google drive\n",
        "        with rasterio.open(img_path + \".tif\", \"w\", **out_meta) as dest:\n",
        "            dest.write(out_image)\n",
        "    except:\n",
        "      print(\"Crop outide of returned images\")\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0UtMd73dhQU"
      },
      "outputs": [],
      "source": [
        "# Return a list of dates to search\n",
        "def get_dates(date_interval, start, end):\n",
        "  dates = []\n",
        "  date = start\n",
        "  while date < end:\n",
        "    dates.append(date)\n",
        "    date = date + np.timedelta64(date_interval, 'M')\n",
        "  return dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFIX4Jb9rQFZ"
      },
      "outputs": [],
      "source": [
        "# Divide an area into multiple geometrys by size\n",
        "def get_geoms(area, size, lat_steps, long_steps):\n",
        "  all_coords = []\n",
        "  start_lat, end_lat =  area[2][1], area[1][1]\n",
        "  start_long, end_long = area[0][0], area[1][0]\n",
        "\n",
        "  for lat in np.linspace(start_lat-size, end_lat, lat_steps):\n",
        "    for long in np.linspace(start_long, end_long-size, long_steps):\n",
        "      all_coords.append( { \n",
        "          \"type\": \"Polygon\",\n",
        "          \"coordinates\": [\n",
        "                    [\n",
        "                      [long, lat],               # lower left\n",
        "                      [long + size, lat],        # lower right\n",
        "                      [long + size, lat + size],               # upper right\n",
        "                      [long, lat + size],                      # upper left\n",
        "                      [long, lat]                # lower left\n",
        "          ]]})\n",
        "  return all_coords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vAoXXEsZUqq"
      },
      "outputs": [],
      "source": [
        "# Perform a search to the API and save images\n",
        "def search_planet(date, num_days, geom, geom_id, res):\n",
        "  date_str = np.datetime_as_string(date)\n",
        "\n",
        "  # Create filters for search\n",
        "  # <10% cloud coverage\n",
        "  cloud_cover_filter = {\n",
        "    \"type\": \"RangeFilter\",\n",
        "    \"field_name\": \"cloud_cover\",\n",
        "    \"config\": {\n",
        "      \"lte\": 0.5\n",
        "    }\n",
        "  }\n",
        "\n",
        "  # Date filter from date and number of days we will search for\n",
        "  lte_date = date + np.timedelta64(num_days, 'D')  # get end date by adding number of days specified\n",
        "  gt_date_str = np.datetime_as_string(date) + 'T00:00:00Z' # format date string for Planet API\n",
        "  lte_date_str = np.datetime_as_string(lte_date) + 'T00:00:00Z'\n",
        "\n",
        "  date_filter = {\n",
        "    \"type\": \"DateRangeFilter\",\n",
        "    \"field_name\": \"acquired\",\n",
        "    \"config\": {\n",
        "      \"gt\": gt_date_str,\n",
        "      \"lte\": lte_date_str\n",
        "    }\n",
        "  }\n",
        "\n",
        "  # Geometry filter from specified coordinates\n",
        "  geometry_filter = {\n",
        "    \"type\": \"GeometryFilter\",\n",
        "    \"field_name\": \"geometry\",\n",
        "    \"config\": geom\n",
        "  }\n",
        "\n",
        "  # Combine filters\n",
        "  combined_filter = {\n",
        "    \"type\": \"AndFilter\",\n",
        "    \"config\": [geometry_filter, cloud_cover_filter, date_filter]\n",
        "  }\n",
        "\n",
        "  # Specify item type \n",
        "  item_types = [\"PSScene3Band\"]\n",
        "\n",
        "  # Create request from item type and  filter\n",
        "  request = {\n",
        "    \"item_types\" : item_types,\n",
        "    \"filter\" : combined_filter\n",
        "  }\n",
        "\n",
        "  # Perform quick search to the API\n",
        "  results=session.post(stats_url, json=request)\n",
        "  quick_url = \"{}/quick-search\".format(URL)\n",
        "  results = session.post(quick_url, json=request)\n",
        "  geojson = results.json()\n",
        "  features = geojson[\"features\"] # Features is a list of dictionaries of all results \n",
        "\n",
        "  # Check if no images are found\n",
        "  if len(features) != 0:\n",
        "    # Images must have the same epsg code to be merged correctly\n",
        "    # Create a list of all epsg codes and only use those with the most common code\n",
        "    features_epsg = []\n",
        "    epsg_codes = []\n",
        "    for f in features:\n",
        "      epsg_codes.append(f[\"properties\"][\"epsg_code\"])\n",
        "    \n",
        "    epsg_codes = Counter(epsg_codes)\n",
        "    epsg = epsg_codes.most_common(1)\n",
        "    for f in features:\n",
        "      if f[\"properties\"][\"epsg_code\"] == epsg[0][0]:\n",
        "        features_epsg.append(f)\n",
        "\n",
        "    max_lat, min_lat, max_long, min_long = get_corners(features_epsg)\n",
        "    \n",
        "    path = \"/content/{}/{}/\".format(geom_id, date_str)\n",
        "    search_id = geom_id + '_' + date_str\n",
        "\n",
        "    # To merge the images into one covering the specified area we need to:\n",
        "    # 1. Save the images as .png\n",
        "    # 2. Use the png image and coordinates from the API results to save corresponding .wld (world files)\n",
        "    # 3. Save the images as .tif and then convert them to geotiff using the .wld files\n",
        "    # 4. Merge the images into one\n",
        "    # 5. Crop the image to only show the search location\n",
        "\n",
        "    # Save imgs as pngs\n",
        "    png_imgs = save_pngs(features_epsg, path, search_id, res)\n",
        "    # Save tif files\n",
        "    tif_imgs = save_transparent_tifs(path)\n",
        "\n",
        "    # add location data to transparent images\n",
        "    imgs_to_merge = \"\"\n",
        "    for feature in features_epsg:\n",
        "      coords = feature['geometry']['coordinates'][0]\n",
        "      ulx = min([coords[0][0], coords[1][0], coords[2][0], coords[3][0]])\n",
        "      uly = max([coords[0][1], coords[1][1], coords[2][1], coords[3][1]])\n",
        "      lrx = max([coords[0][0], coords[1][0], coords[2][0], coords[3][0]])\n",
        "      lry = min([coords[0][1], coords[1][1], coords[2][1], coords[3][1]])\n",
        "      current_tif = path + str(feature[\"id\"]) + \"_transparent.tif\"\n",
        "      new_tif = path + str(feature[\"id\"]) + \".tif\"\n",
        "      #!gdal_translate -a_ullr $ulx $uly $lrx $lry $current_tif $new_tif\n",
        "      cmd = \"gdal_translate -a_ullr {} {} {} {} {} {}\".format(ulx, uly, lrx, lry, current_tif, new_tif)\n",
        "      call(cmd.split(\" \"))\n",
        "      imgs_to_merge += new_tif + \" \"\n",
        "    \n",
        "    imgs_to_merge = imgs_to_merge[:-1]\n",
        "    merged_img =  path + search_id + '.tif' # Set file location for the merged image\n",
        " \n",
        "    # Merge images into one .tiff file\n",
        "    !rio merge $imgs_to_merge --output $merged_img\n",
        "\n",
        "    # Change the coordinates of the merged image\n",
        "    !gdal_edit.py -a_ullr $min_long $max_lat $max_long $min_lat $merged_img\n",
        "    \n",
        "    # crop image to each adjacent area\n",
        "    crop_img(geom, merged_img, path + search_id + \"_cropped\")\n",
        "\n",
        "    # save cropped image \n",
        "    cropped_img = cv2.imread(path + search_id + \"_cropped.tif\")\n",
        "    cv2.imwrite(\"/content/drive/My Drive/Colab Notebooks/FYP/Sequences1/\" + search_id + \".tif\", cropped_img)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWgrODs5a8Hl"
      },
      "outputs": [],
      "source": [
        "# Create sequence of images\n",
        "\n",
        "# Set parameters\n",
        "date_interval = 3   # Number of months between stage in sequence\n",
        "start = np.datetime64('2019-01')  # Date to start sequence\n",
        "end = np.datetime64('2022-04')    # Date to end sequence\n",
        "num_days = 50        # Number of days to collect images\n",
        "size = 1         # Size to divide each area by\n",
        "res = 512        # Resolution of thumbnails images\n",
        "classify_model = load_model(\"/content/drive/My Drive/Colab Notebooks/FYP/classification_model.h5\", custom_objects={\"f2_score\": f2_score})\n",
        "\n",
        "# Get list of dates to search\n",
        "dates = get_dates(date_interval, start, end)\n",
        "# Get locations\n",
        "area = [\n",
        "        [-68, -14], \n",
        "        [-50, -14],\n",
        "        [-50, 0],\n",
        "        [-68, 0],\n",
        "        [-68, -14]\n",
        "    ]\n",
        "\n",
        "geoms = get_geoms(area, size, 14, 18)\n",
        "print(len(geoms))\n",
        "num_sequences = 100\n",
        "\n",
        "# Iterate through locations, to create sequence of images\n",
        "for g, geom in enumerate(geoms):\n",
        "  print(\"Searching location: {}/{}\".format(g, num_sequences))\n",
        "  geom_images = np.zeros((13, 400, 64, 64, 3))\n",
        "  if not os.path.exists('/content/' + str(g) + '/'):\n",
        "    os.mkdir('/content/' + str(g) + '/') # Create directory for geom\n",
        "  for d, date in enumerate(dates):\n",
        "    date = date + np.timedelta64(0, 'D')\n",
        "    str_date = np.datetime_as_string(date)\n",
        "    print(\"Searching date: \" + str_date)\n",
        "    if not os.path.exists('/content/{}/{}/'.format(str(g), str_date)):\n",
        "      os.mkdir('/content/{}/{}/'.format(str(g), str_date))  # Create directory for date\n",
        "    if not os.path.exists(\"/content/{}/{}/{}_{}_cropped.tif\".format(g, str_date, g, str_date)):\n",
        "      search_planet(date, num_days, geom, str(g), res)\n",
        "    search_id = str(g) + \"_\" + str_date\n",
        "    cropped_img = \"/content/{}/{}/{}_cropped.tif\".format(g, str_date, search_id)\n",
        "    current_area = geom['coordinates'][0]\n",
        "    cropped_img = cv2.imread(cropped_img)\n",
        "    ulx, uly, lrx, lry = current_area[3][0], current_area[3][1], current_area[1][0], current_area[1][1]\n",
        "    !gdal_edit.py -a_ullr $ulx $uly $lrx $lry $cropped_img\n",
        "    # crop images and make predictions to build map layer\n",
        "    sub_path = '/content/{}/{}/sub_geoms/'.format(str(g), str_date)\n",
        "    if not os.path.exists(sub_path):\n",
        "      os.mkdir(sub_path)\n",
        "    sub_geoms = get_geoms(current_area, 0.05, 20, 20)\n",
        "    for sg, sub_geom in enumerate(sub_geoms):\n",
        "      img_path = sub_path + search_id + \"_\" + str(sg)\n",
        "      if not os.path.exists(img_path + \".tif\"):\n",
        "        crop_img(sub_geom, cropped_img_path2, img_path)\n",
        "        img = cv2.imread(img_path + \".tif\")\n",
        "        img = cv2.resize(img, (64, 64))\n",
        "        geom_images[d, sg] = img\n",
        "  np.save(\"/content/drive/My Drive/Colab Notebooks/FYP/sequence training data/\" + str(g), geom_images)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Generate Sequences.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}